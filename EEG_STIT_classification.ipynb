{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fze7OQ5yeU35",
        "outputId": "9132c62d-2caa-493d-bbfc-b1c42b739a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Revision_Paper1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Revision_Paper1/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YI0Yxc1lebAs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path1 = '/content/drive/My Drive/Revision_Paper1/beta_wa2.mat'\n",
        "data1 = loadmat(file_path1)[\"reshape\"]\n",
        "file_path2 = '/content/drive/My Drive/Revision_Paper1/beta_wb2.mat'\n",
        "data2 = loadmat(file_path2)[\"reshape\"]\n",
        "file_path3 = '/content/drive/My Drive/Revision_Paper1/beta_wc2.mat'\n",
        "data3 = loadmat(file_path3)[\"reshape\"]\n",
        "file_path4 = '/content/drive/My Drive/Revision_Paper1/beta_wd2.mat'\n",
        "data4 = loadmat(file_path4)[\"reshape\"]"
      ],
      "metadata": {
        "id": "XQ8ywzxAemET"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def standardize_data(*datasets):\n",
        "    standardized = []\n",
        "    for data in datasets:\n",
        "        mean = data.mean(axis=0)\n",
        "        std = data.std(axis=0)\n",
        "        standardized_data = (data - mean) / std\n",
        "        standardized.append(standardized_data)\n",
        "    return standardized\n"
      ],
      "metadata": {
        "id": "2KZ7T3DWemG6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1, d2, d3, d4 = standardize_data(data1, data2, data3, data4)\n"
      ],
      "metadata": {
        "id": "OjNsahaSemKW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_array = np.concatenate([d1, d2, d3, d4])\n",
        "n = data_array.shape[0] // 4\n",
        "\n",
        "labels = np.concatenate((\n",
        "    np.zeros(n),\n",
        "    np.ones(n),\n",
        "    np.full(n, 2),\n",
        "    np.full(n, 3)\n",
        "))\n"
      ],
      "metadata": {
        "id": "gdOE6iR0fpeM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense, TimeDistributed, Reshape\n",
        "from keras.layers import GlobalAveragePooling1D, Conv2D, GRU, AveragePooling2D, DepthwiseConv1D, SeparableConv1D, LSTM, MaxPool1D, MaxPooling2D, AveragePooling2D,Conv1D\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input,  concatenate, add\n",
        "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense, Embedding, Layer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "M4FE3XswgQ6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "PXHJ2dx5h1bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Set up cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracy = []\n",
        "fold_loss = []\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(x, y)):\n",
        "    print(f\"Training fold {fold + 1}...\")\n",
        "\n",
        "    # Split data for current fold\n",
        "    x_train_fold, x_val_fold = x[train_index], x[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    # Define the model architecture within the loop for a fresh model each fold\n",
        "    input_layer = Input(shape=(656, 21, 1))\n",
        "    conv1 = Conv2D(15, (21, 1), activation='relu', padding='same')(input_layer)\n",
        "    conv = Conv2D(15, (1, 20), activation='relu', padding='same')(conv1)\n",
        "    conv1a = Conv2D(16, (1, 1), activation='relu', padding='same')(conv)\n",
        "    max1 = MaxPooling2D((2, 2), padding='same')(conv1a)\n",
        "    conv3a = Conv2D(32, (3, 3), activation='relu', padding='same')(conv)\n",
        "    max2 = MaxPooling2D((2, 2), padding='same')(conv3a)\n",
        "    conv5a = Conv2D(64, (5, 5), activation='relu', padding='same')(conv)\n",
        "    max3 = MaxPooling2D((2, 2), padding='same')(conv5a)\n",
        "    concat = concatenate([max1, max2, max3], axis=-1)\n",
        "    maxpool = MaxPooling2D((3, 3), padding='valid')(concat)\n",
        "    reshaped = Reshape((maxpool.shape[1] * maxpool.shape[2], maxpool.shape[3]))(maxpool)\n",
        "\n",
        "    # Transformer encoder\n",
        "    transformer_layer = transformer_encoder(inputs=reshaped, head_size=128, num_heads=3, ff_dim=128, dropout=0.5)\n",
        "    output_layer = Flatten()(transformer_layer)\n",
        "    output_layer = Dense(4, activation='softmax')(output_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        validation_data=(x_val_fold, y_val_fold),\n",
        "        epochs=50,\n",
        "        batch_size=16,\n",
        "        verbose=1\n",
        "    )\n",
        "    val_loss, val_accuracy = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
        "    fold_accuracy.append(val_accuracy)\n",
        "    fold_loss.append(val_loss)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_fold = np.argmax(model.predict(x_val_fold), axis=1)\n",
        "    all_y_true.extend(y_val_fold)\n",
        "    all_y_pred.extend(y_pred_fold)\n",
        "\n",
        "# Calculate overall metrics\n",
        "conf_matrix = confusion_matrix(all_y_true, all_y_pred)\n",
        "precision = precision_score(all_y_true, all_y_pred, average='weighted')\n",
        "recall = recall_score(all_y_true, all_y_pred, average='weighted')\n",
        "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
        "\n",
        "# Output results\n",
        "print(f\"\\nAverage validation accuracy across folds: {np.mean(fold_accuracy)}\")\n",
        "print(f\"Average validation loss across folds: {np.mean(fold_loss)}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "XWTqolyShPEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_accuracy_per_fold = []\n",
        "val_accuracy_per_fold = []\n",
        "\n",
        "for history in history_list:\n",
        "    train_accuracy_per_fold.append(history['accuracy'])\n",
        "    val_accuracy_per_fold.append(history['val_accuracy'])\n",
        "\n",
        "avg_train_accuracy = np.mean(train_accuracy_per_fold, axis=0)\n",
        "avg_val_accuracy = np.mean(val_accuracy_per_fold, axis=0)\n",
        "\n",
        "# Plot the averaged accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.ylim(0,1)\n",
        "plt.yticks(np.arange(0, 1.1,0.1))\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(avg_train_accuracy, label='Average Train Accuracy', linestyle='-', color='b')\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.plot(avg_val_accuracy, label='Average Val Accuracy', linestyle='--', color='r')\n",
        "\n",
        "plt.title('Average Accuracy Across Folds_Beta MA_2 class')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bYuOp-TPhY3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}